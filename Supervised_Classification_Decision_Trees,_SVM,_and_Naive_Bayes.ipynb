{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Information Gain, and how is it used in Decision Trees?\n",
        ">- Information Gain is a metric that measures how much uncertainty (entropy) is reduced after splitting a dataset on a particular feature.\n",
        "\n",
        "Use in Decision Trees:\n",
        "In a decision tree, Information Gain is used to select the best feature for splitting at each node—the feature with the highest Information Gain creates the most effective split and improves classification accuracy.\n",
        "\n",
        "Q2. What is the difference between Gini Impurity and Entropy?\n",
        ">- Gini Impurity: Measures how often a randomly chosen sample would be incorrectly classified. It is faster to compute and commonly used in CART decision trees.\n",
        "\n",
        "Entropy: Measures the level of uncertainty or disorder in the data using logarithms. It is more computationally expensive and used in ID3/C4.5 trees.\n",
        "\n",
        "Key point: Both are criteria to select the best split; Gini is faster, Entropy is more informative but slower.\n",
        "\n",
        "Q3. What is Pre-Pruning in Decision Trees?\n",
        ">- Pre-Pruning is a technique in decision trees where the tree’s growth is stopped early by setting conditions like maximum depth, minimum samples per node, or minimum information gain, to prevent overfitting.\n",
        "\n",
        "Q4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        ">-"
      ],
      "metadata": {
        "id": "Fs_YHToVX1x7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Cu4-8DFr0B",
        "outputId": "b3e43458-f4d2-4000-97f4-8de501d218e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.  What is a Support Vector Machine (SVM)?\n",
        ">- A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression that finds the optimal hyperplane separating data points of different classes by maximizing the margin between the closest points (support vectors).\n",
        "\n",
        "Q6. What is the Kernel Trick in SVM?\n",
        ">- The Kernel Trick in SVM is a technique that allows data to be mapped into a higher-dimensional space without explicitly computing the transformation, enabling SVMs to separate non-linearly separable data using kernels like linear, polynomial, or RBF.\n",
        "\n",
        "Q7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "KPBKcUyEY6VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Linear SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Linear SVM Accuracy:\", acc_linear)\n",
        "print(\"RBF SVM Accuracy:\", acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFe4fiFGY3k5",
        "outputId": "be391191-f540-4c18-9602-f797b6fe645e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9814814814814815\n",
            "RBF SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        ">- The Naïve Bayes classifier is a probabilistic classification algorithm based on Bayes’ Theorem. It is called “Naïve” because it assumes that all features are independent of each other, which is a strong (naïve) assumption in real-world data.\n",
        "\n",
        "Q9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes?\n",
        ">- Differences between Naïve Bayes types:\n",
        "\n",
        "Gaussian Naïve Bayes: Assumes features follow a normal (Gaussian) distribution; used for continuous data.\n",
        "\n",
        "Multinomial Naïve Bayes: Used for discrete count data (e.g., word frequencies in text).\n",
        "\n",
        "Bernoulli Naïve Bayes: Used for binary features (0/1), focusing on presence or absence of features.\n",
        "\n",
        "Q10. Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "2gyiGfi7ZjgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naïve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX9lQxseaM8_",
        "outputId": "eacec07c-4d97-46e8-e779-c91495d3d898"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}